{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with AWS: A Demo with Tires and Licence Plates\n",
    "\n",
    "This series of notebooks demonstrates tackling a sample computer vision problem on AWS - building a two-class object detector.\n",
    "\n",
    "**This notebook** walks through using the [SageMaker Ground Truth](https://aws.amazon.com/sagemaker/groundtruth/) tool to annotate training and validation data sets.\n",
    "\n",
    "**Follow-on** notebooks show how to train a range of models from the created dataset, including:\n",
    "\n",
    "* [Amazon Rekognition](https://aws.amazon.com/rekognition/)'s new [custom labels](https://aws.amazon.com/rekognition/custom-labels-features/) functionality, announced at Re:Invent 2019\n",
    "* SageMaker's [built-in object detection algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html)\n",
    "\n",
    "# Tires and Plates 1: Introduction and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "We use the [**Open Images Dataset v4**](https://storage.googleapis.com/openimages/web/download_v4.html) as a convenient source of pre-curated images. The Open Images Dataset V4 is created by Google Inc. We have not modified the images or the accompanying annotations. You can obtain the images and the annotations [here](https://storage.googleapis.com/openimages/web/download_v4.html). The annotations are licensed by Google Inc. under CC BY 4.0 license. The images are listed as having a CC BY 2.0 license. The following paper describes Open Images V4 in depth: from the data collection and annotation to detailed statistics about the data and evaluation of models trained on it.\n",
    "\n",
    "A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. Pont-Tuset, S. Kamali, S. Popov, M. Malloci, T. Duerig, and V. Ferrari. The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale. arXiv:1811.00982, 2018. ([link to PDF](https://arxiv.org/abs/1811.00982))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "This notebook is designed to be run in Amazon SageMaker. To run it (and understand what's going on), you'll need:\n",
    "\n",
    "* Basic familiarity with Python, [AWS S3](https://docs.aws.amazon.com/s3/index.html), [Amazon Sagemaker](https://aws.amazon.com/sagemaker/), and the [AWS Command Line Interface (CLI)](https://aws.amazon.com/cli/).\n",
    "* To run in **a region where Rekognition custom labelling is available** (Only N. Virginia at launch), if you plan to explore this feature.\n",
    "* To create an **S3 bucket** in the same region, and ensure the SageMaker notebook's role has access to this bucket.\n",
    "* Sufficient [SageMaker quota limits](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html#limits_sagemaker) set on your account to run GPU-accelerated training jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost and runtime\n",
    "\n",
    "Depending on your configuration, this demo may consume resources outside of the free tier but should not generally be expensive because we'll be training on a small number of images. You might wish to review the following for your region:\n",
    "\n",
    "* [Amazon SageMaker pricing](https://aws.amazon.com/sagemaker/pricing/)\n",
    "* [SageMaker Ground Truth pricing](https://aws.amazon.com/sagemaker/groundtruth/pricing/)\n",
    "* [Amazon Rekognition pricing](https://aws.amazon.com/rekognition/pricing/)\n",
    "\n",
    "The standard `ml.t2.medium` instance should be sufficient to run the notebooks.\n",
    "\n",
    "We will use GPU-accelerated instance types for training and hyperparameter optimization, and use spot instances where appropriate to optimize these costs.\n",
    "\n",
    "As noted in the step-by-step guidance, you should take particular care to delete any created SageMaker real-time prediction endpoints when finishing the demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Dependencies and configuration\n",
    "\n",
    "As usual we'll start by loading libraries, defining configuration, and connecting to the AWS SDKs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# Built-Ins:\n",
    "import csv\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "import imageio\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Local Dependencies:\n",
    "%aimport util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we configure the name and layout of your bucket, and the annotation job to set up.\n",
    "\n",
    "**If you're following this demo in a group:**, you can pool your annotations for better accuracy without spending hours annotating:\n",
    "\n",
    "* Have each group member set a different `BATCH_OFFSET` integer from 0 upwards and you'll be allocated different images to annotate\n",
    "* Later, you can *import* the other members' output manifest files to your own S3 data set.\n",
    "\n",
    "**If not:** don't worry - we already provide a 100-image set in this repository to augment your annotations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'BUCKET_NAME' (str)\n",
      "Stored 'DATA_PREFIX' (str)\n",
      "Stored 'MODELS_PREFIX' (str)\n",
      "Stored 'CHECKPOINTS_PREFIX' (str)\n",
      "Stored 'CLASS_NAMES' (list)\n",
      "Stored 'test_image_folder' (str)\n"
     ]
    }
   ],
   "source": [
    "## Overall S3 bucket layout:\n",
    "# Note you could instead set for auto-setup: BUCKET_NAME = sagemaker.Session().default_bucket()\n",
    "BUCKET_NAME = 'vtg-20200124-gt-demo'\n",
    "%store BUCKET_NAME\n",
    "DATA_PREFIX = \"data\" # The folder in the bucket (and locally) where we will store data\n",
    "%store DATA_PREFIX\n",
    "MODELS_PREFIX = \"models\" # The folder in the bucket where we will store models\n",
    "%store MODELS_PREFIX\n",
    "CHECKPOINTS_PREFIX = \"models/checkpoints\" # Model checkpoints can go in a subfolder of models\n",
    "%store CHECKPOINTS_PREFIX\n",
    "\n",
    "## Annotation job:\n",
    "CLASS_NAMES = [\"Tire\", \"Vehicle registration plate\"]\n",
    "%store CLASS_NAMES\n",
    "N_EXAMPLES_PER_CLASS = 20\n",
    "BATCH_OFFSET = 0\n",
    "BATCH_NAME = \"my-annotations\"\n",
    "\n",
    "# Note that some paths are reserved, restricting your choice of BATCH_NAME:\n",
    "data_raw_prefix = DATA_PREFIX + \"/raw\"\n",
    "data_augment_prefix = DATA_PREFIX + \"/augmentation\"\n",
    "data_batch_prefix = f\"{DATA_PREFIX}/{BATCH_NAME}\"\n",
    "test_image_folder = DATA_PREFIX + \"/test\"\n",
    "%store test_image_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just connect to the AWS SDKs we'll use, and validate the choice of S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "s3 = session.resource(\"s3\")\n",
    "bucket = s3.Bucket(BUCKET_NAME)\n",
    "smclient = session.client(\"sagemaker\")\n",
    "\n",
    "bucket_region = \\\n",
    "    session.client(\"s3\").head_bucket(Bucket=BUCKET_NAME)[\"ResponseMetadata\"][\"HTTPHeaders\"][\"x-amz-bucket-region\"]\n",
    "assert (\n",
    "    bucket_region == region\n",
    "), f\"Your S3 bucket {BUCKET_NAME} and this notebook need to be in the same region.\"\n",
    "\n",
    "if (region != \"us-east-1\"):\n",
    "    print(\"WARNING: Rekognition Custom Labels functionality is only available in us-east-1 at launch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set the goalposts with some unlabelled target data\n",
    "\n",
    "Let's start out by collecting a handful of images from around the web to illustrate what we'd like to detect.\n",
    "\n",
    "These images are not licensed and the links may break for different regions / times in future: Feel free to add your own or replace with any other images of boots and cats!\n",
    "\n",
    "Model evaluations in following notebooks will loop through each image in the `test_image_folder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Map our class names to OpenImages class IDs\n",
    "\n",
    "OpenImages defines a hierarchy of object types (e.g. \"swan\" is a subtype of \"bird\"), and references each with a class ID instead of the human-readable name.\n",
    "\n",
    "Since we want to find images containing boots and cats, our first job is to figure what OpenImages class IDs they correspond to.\n",
    "\n",
    "We start by downloading the OpenImages metadata, below.\n",
    "\n",
    "(Note we're only referencing the `test` subset of OpenImages as an easy way to keep data volumes small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-01-23 10:10:58--  https://storage.googleapis.com/openimages/2018_04/test/test-annotations-bbox.csv\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.12.240, 2607:f8b0:4004:807::2010\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.12.240|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 52174204 (50M) [text/csv]\n",
      "Saving to: ‘data/raw/annotations-bbox.csv’\n",
      "\n",
      "data/raw/annotation 100%[===================>]  49.76M   124MB/s    in 0.4s    \n",
      "\n",
      "2020-01-23 10:10:58 (124 MB/s) - ‘data/raw/annotations-bbox.csv’ saved [52174204/52174204]\n",
      "\n",
      "--2020-01-23 10:10:58--  https://storage.googleapis.com/openimages/2018_04/class-descriptions.csv\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.12.240, 2607:f8b0:4004:807::2010\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.12.240|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 475854 (465K) [text/csv]\n",
      "Saving to: ‘data/raw/class-descriptions.csv’\n",
      "\n",
      "data/raw/class-desc 100%[===================>] 464.70K  --.-KB/s    in 0.007s  \n",
      "\n",
      "2020-01-23 10:10:58 (62.7 MB/s) - ‘data/raw/class-descriptions.csv’ saved [475854/475854]\n",
      "\n",
      "--2020-01-23 10:10:59--  https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.12.240, 2607:f8b0:4004:807::2010\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.12.240|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 86291 (84K) [text/csv]\n",
      "Saving to: ‘data/raw/labels-hierarchy.json’\n",
      "\n",
      "data/raw/labels-hie 100%[===================>]  84.27K  --.-KB/s    in 0.003s  \n",
      "\n",
      "2020-01-23 10:10:59 (30.3 MB/s) - ‘data/raw/labels-hierarchy.json’ saved [86291/86291]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download and process the Open Images annotations.\n",
    "os.makedirs(data_raw_prefix, exist_ok=True)\n",
    "!wget -O $data_raw_prefix/annotations-bbox.csv https://storage.googleapis.com/openimages/2018_04/test/test-annotations-bbox.csv \n",
    "!wget -O $data_raw_prefix/class-descriptions.csv https://storage.googleapis.com/openimages/2018_04/class-descriptions.csv \n",
    "!wget -O $data_raw_prefix/labels-hierarchy.json https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a case-insensitive lookup for our class names in the classes CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class name -> root ID mapping done\n",
      "{'Tire': '/m/0h9mv', 'Vehicle registration plate': '/m/01jfm_'}\n"
     ]
    }
   ],
   "source": [
    "# The class list is really long, so let's stream it instead of loading dataframe:\n",
    "class_root_ids = { s: None for s in CLASS_NAMES }\n",
    "classes_lower_notfound = { s.lower(): s for s in CLASS_NAMES }\n",
    "with open(f\"{data_raw_prefix}/class-descriptions.csv\", \"r\") as f:\n",
    "    for row in csv.reader(f):\n",
    "        row_class_lower = row[1].lower()\n",
    "        match = classes_lower_notfound.get(row_class_lower)\n",
    "        if (match is not None):\n",
    "            class_root_ids[match] = row[0]\n",
    "            del classes_lower_notfound[row_class_lower]\n",
    "            if (len(classes_lower_notfound) == 0):\n",
    "                print(\"Class name -> root ID mapping done\")\n",
    "                break\n",
    "\n",
    "print(class_root_ids)\n",
    "if len(classes_lower_notfound):\n",
    "    raise ValueError(\n",
    "        f\"IDs not found for these class names: {[v for (k,v) in classes_lower_notfound.items()]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we recurse down the ontology from these root classes to capture any child classes.\n",
    "\n",
    "(Note that actually \"boot\" and \"cat\" are leaf nodes in OpenImages v4, but other common demos like \"bird\" are not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final OpenImages class ID sets:\n",
      "{'Tire': {'/m/0h9mv'}, 'Vehicle registration plate': {'/m/01jfm_'}}\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{data_raw_prefix}/labels-hierarchy.json\", \"r\") as f:\n",
    "    hierarchy = json.load(f)\n",
    "\n",
    "def get_all_subclasses(class_id, tree):\n",
    "    \"\"\"Get the set of `class_id` and all matching subclasses from hierarchy `tree`\"\"\"\n",
    "    def all_subtree_class_ids(subtree):\n",
    "        if (\"Subcategory\" in subtree):\n",
    "            return set([subtree[\"LabelName\"]]).union(\n",
    "                *[all_subtree_class_ids(s) for s in subtree[\"Subcategory\"]]\n",
    "            )\n",
    "        else:\n",
    "            return set([subtree[\"LabelName\"]])\n",
    "    if (tree[\"LabelName\"] == class_id):\n",
    "        return all_subtree_class_ids(tree)\n",
    "    elif \"Subcategory\" in tree:\n",
    "        return set().union(*[get_all_subclasses(class_id, s) for s in tree[\"Subcategory\"]])\n",
    "    else:\n",
    "        return set()\n",
    "\n",
    "class_id_sets = {\n",
    "    name: get_all_subclasses(class_root_ids[name], hierarchy) for name in class_root_ids\n",
    "}\n",
    "print(\"Final OpenImages class ID sets:\")\n",
    "print(class_id_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Find suitable example images\n",
    "\n",
    "Now we've looked up the full range of applicable label IDs, we can use the OpenImages annotations to extract which image IDs will be interesting for us to train on (i.e. they contain boots and/or cats).\n",
    "\n",
    "We deliberately search through the data-set in deterministic order, and only want to collect `N_EXAMPLES_PER_CLASS` images for each label but need to offset the ones we pick up by `BATCH_OFFSET` if this is non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 38 images\n"
     ]
    }
   ],
   "source": [
    "# Skip these images with known bad quality content:\n",
    "SKIP_IMAGES = {\"251d4c429f6f9c39\", \"065ad49f98157c8d\"}\n",
    "\n",
    "# Dict[class_name][img_id] -> [class_name, xmin, xmax, ymin, ymax]\n",
    "class_bbs = { name: defaultdict(list) for name in class_id_sets }\n",
    "\n",
    "# BATCH_OFFSET allows \n",
    "n_images_needed = N_EXAMPLES_PER_CLASS * (BATCH_OFFSET + 1)\n",
    "\n",
    "unfilled_class_names = set(CLASS_NAMES)\n",
    "with open(f\"{data_raw_prefix}/annotations-bbox.csv\", \"r\") as f:\n",
    "    for row in csv.reader(f):\n",
    "        img_id, _, cls_id, conf, xmin, xmax, ymin, ymax, *_ = row\n",
    "        if (img_id in SKIP_IMAGES):\n",
    "            continue\n",
    "        curr_unfilled_class_names = unfilled_class_names.copy()\n",
    "        for name in curr_unfilled_class_names:\n",
    "            if (cls_id in class_id_sets[name]):\n",
    "                class_bbs[name][img_id].append([name, xmin, xmax, ymin, ymax])\n",
    "                if (len(class_bbs[name]) >= n_images_needed):\n",
    "                    unfilled_class_names.remove(name)\n",
    "                    \n",
    "if (len(unfilled_class_names)):\n",
    "    print(\n",
    "        \"WARNING: Found fewer than (\"\n",
    "        + f\"{N_EXAMPLES_PER_CLASS}x{BATCH_OFFSET+1}={n_images_needed}\"\n",
    "        + \") requested images for the following classes:\\n\"\n",
    "        + \"\\n\".join([f\"{name} ({len(class_bbs[name])} images)\" for name in unfilled_class_names])\n",
    "    )\n",
    "\n",
    "bbs = defaultdict(list)\n",
    "for class_name in class_bbs:\n",
    "    # Take last N_EXAMPLES_PER_CLASS images from each class (for BATCH_OFFSET)\n",
    "    class_bbs_all_unfiltered = list(class_bbs[class_name].items())\n",
    "    class_bbs_batch = class_bbs_all_unfiltered[-N_EXAMPLES_PER_CLASS:]\n",
    "    class_bbs[class_name] = defaultdict(list, class_bbs_batch)\n",
    "    # Concatenate each class together into the overall `bbs` set\n",
    "    for (img_id, boxes) in class_bbs_batch:\n",
    "        bbs[img_id] = bbs[img_id] + boxes\n",
    "\n",
    "image_ids = bbs.keys()\n",
    "n_images = len(image_ids)\n",
    "print(f\"Selected {n_images} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Upload images and manifest file to S3\n",
    "\n",
    "We need our training image data in an accessible S3 bucket, and a [manifest](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-data-input.html) file defining for SageMaker Ground Truth (and later our model) what images are in the data set and where to find them.\n",
    "\n",
    "In the following cell, we:\n",
    "\n",
    "* Copy each identified image directly from the OpenImages repository to our bucket\n",
    "* Build up a local manifest file listing all the images\n",
    "* Upload the manifest file to the bucket\n",
    "\n",
    "This process should only take a few seconds with small data sets like we're dealing with here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying images......................................\n",
      "Images copied to s3://vtg-20200124-gt-demo/data/my-annotations/images/\n",
      "Manifest uploaded to s3://vtg-20200124-gt-demo/data/my-annotations/manifests/input.manifest\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(f\"{data_batch_prefix}/manifests\", exist_ok=True)\n",
    "input_manifest_loc = f\"{data_batch_prefix}/manifests/input.manifest\"\n",
    "\n",
    "with open(input_manifest_loc, \"w\") as f:\n",
    "    print(\"Copying images\", end=\"\")\n",
    "    # TODO: Delete existing folder contents?\n",
    "    for image_id in image_ids:\n",
    "        print(\".\", end=\"\")\n",
    "        dest_key = f\"{data_batch_prefix}/images/{image_id}.jpg\"\n",
    "        bucket.copy(\n",
    "            {\n",
    "                \"Bucket\": \"open-images-dataset\",\n",
    "                \"Key\": f\"test/{image_id}.jpg\"\n",
    "            },\n",
    "            dest_key\n",
    "        )\n",
    "        f.write(json.dumps({ \"source-ref\": f\"s3://{BUCKET_NAME}/{dest_key}\" }) + \"\\n\")\n",
    "    print(\"\")\n",
    "    print(f\"Images copied to s3://{BUCKET_NAME}/{data_batch_prefix}/images/\")\n",
    "\n",
    "bucket.upload_file(input_manifest_loc, input_manifest_loc)\n",
    "print(f\"Manifest uploaded to s3://{BUCKET_NAME}/{input_manifest_loc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Set up the SageMaker Ground Truth labelling job\n",
    "\n",
    "Now that our images and a manifest file listing them are ready in S3, we'll set up the Ground Truth labelling job **in the [AWS console](https://console.aws.amazon.com)**.\n",
    "\n",
    "Under *Services* go to *Amazon SageMaker*, and select *Ground Truth > Labeling Jobs* from the side-bar menu on the left.\n",
    "\n",
    "**Note:** These steps assume you've either never used SageMaker Ground Truth before, or have already set up a Private Workforce that will be suitable for this task. If you have one or more private workforces configured already, but none of them are appropriate for this task, you'll need to go to *Ground Truth > Labeling workforces* **first** to create a new one.\n",
    "\n",
    "### Job Details\n",
    "\n",
    "Click the **Create labeling job** button, and you'll be asked to specify job details as follows:\n",
    "\n",
    "* **Job name:** Choose a name to identify this labelling job, e.g. `boots-and-cats-batch-0`\n",
    "* **Label name (The override checkbox):** Consider overriding this to `labels`\n",
    "* **Input data location:** The path to the input manifest file in S3 (see output above)\n",
    "* **Output data location:** Set this just to the parent folder of the input manifests (e.g. *s3://gt-object-detect-thewsey-us-east-1/data/my-annotations*)\n",
    "* **IAM role:** If you're not sure whether your existing roles have the sufficient permissions for Ground Truth, select the options to create a new role\n",
    "* **Task type:** Image > Bounding box\n",
    "\n",
    "<img src=\"BlogImages/JobDetailsIntro.png\"/>\n",
    "\n",
    "All other settings can be left as default. Record your choices for the label name and output data location below, because we'll need these later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_groundtruth_job_name = 'vtg-demo-2'\n",
    "my_groundtruth_output = f\"s3://{BUCKET_NAME}/data/my-annotations\" # TODO: **No trailing slash!**\n",
    "my_groundtruth_labels = \"labels\" # TODO: Check this matches yours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workers\n",
    "\n",
    "On the next screen, we'll configure **who** will annotate our data: Ground Truth allows you to define your own in-house *Private Workforces*; use *Vendor Managed Workforces* for specialist tasks; or use the public workforce provided by *Amazon Mechanical Turk*.\n",
    "\n",
    "Select **Private** worker type, and you'll be prompted either to select from your existing private workforces, or create a new one if none exist.\n",
    "\n",
    "To create a new private workforce if you need, simply follow the UI workflow with default settings. It doesn't matter what you call the workforce, and you can create a new Cognito User Group to define the workforce. **Add yourself** to the user pool by adding your email address: You should receive a confirmation email shortly with a temporary password and a link to access the annotation portal.\n",
    "\n",
    "Automatic data labeling is applicable only for data sets over 1000 samples, so leave this turned **off** for now.\n",
    "\n",
    "<img src=\"BlogImages/SelectPrivateWorkforce.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling Tool\n",
    "\n",
    "Since you'll be labelling the data yourself, a brief description of the task should be fine in this case. When using real workforces, it's important to be really clear in this section about the task requirements and best practices - to ensure consistency of annotations between human workers.\n",
    "\n",
    "For example: In the common case where we see a *pair* of boots from the side and one is almost entirely obscured, how should the image be annotated? Should *model* cats count, or only real ones?\n",
    "\n",
    "The most important configuration here is to set the *options* to be the same as our `CLASS_NAMES` and in the same order: **Boot, Cat**\n",
    "\n",
    "<img src=\"BlogImages/LabellingToolSetup.png\"/>\n",
    "\n",
    "Take some time to explore the other options for configuring the annotation tool; and when you're ready click \"Create\" to launch the labeling job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Label those images!\n",
    "\n",
    "Follow the link you received in your workforce invitation email to the workforce's **labelling portal**, and log in with the default password given in the email (which you'll be asked to change).\n",
    "\n",
    "If you lose the portal link, you can always retrieve it through the *Ground Truth > Labeling Workforces* menu in the SageMaker console: Near the top of the summary of private workforces.\n",
    "\n",
    "New jobs can sometimes take a minute or two to appear for workers, but you should soon see a screen like the below. Select the job and click \"Start working\" to enter the labelling tool.\n",
    "\n",
    "<img src=\"BlogImages/LabellingJobsReady.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can check on the progress of labelling jobs through the APIs as well as in the AWS console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Completed'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smclient.describe_labeling_job(LabelingJobName=my_groundtruth_job_name)['LabelingJobStatus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label all the images in the tool by selecting the class and drawing boxes around the objects, and when done you will be brought back to the (now empty) jobs list screen above.\n",
    "\n",
    "It may take a few seconds after completing for the job status to update in the AWS console."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
