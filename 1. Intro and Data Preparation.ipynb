{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with AWS: A Demo with Boots and Cats\n",
    "\n",
    "This series of notebooks demonstrates tackling a sample computer vision problem on AWS - building a two-class object detector for [boots and cats](https://www.youtube.com/watch?v=Nni0rTLg5B8).\n",
    "\n",
    "**This notebook** walks through using the [SageMaker Ground Truth](https://aws.amazon.com/sagemaker/groundtruth/) tool to annotate training and validation data sets.\n",
    "\n",
    "**Follow-on** notebooks show how to train a range of models from the created dataset, including:\n",
    "\n",
    "* [Amazon Rekognition](https://aws.amazon.com/rekognition/)'s new [custom labels](https://aws.amazon.com/rekognition/custom-labels-features/) functionality, announced at Re:Invent 2019\n",
    "* SageMaker's [built-in object detection algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html)\n",
    "\n",
    "# Boots 'n' Cats 1: Introduction and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "We use the [**Open Images Dataset v4**](https://storage.googleapis.com/openimages/web/download_v4.html) as a convenient source of pre-curated images. The Open Images Dataset V4 is created by Google Inc. We have not modified the images or the accompanying annotations. You can obtain the images and the annotations [here](https://storage.googleapis.com/openimages/web/download_v4.html). The annotations are licensed by Google Inc. under CC BY 4.0 license. The images are listed as having a CC BY 2.0 license. The following paper describes Open Images V4 in depth: from the data collection and annotation to detailed statistics about the data and evaluation of models trained on it.\n",
    "\n",
    "A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. Pont-Tuset, S. Kamali, S. Popov, M. Malloci, T. Duerig, and V. Ferrari. The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale. arXiv:1811.00982, 2018. ([link to PDF](https://arxiv.org/abs/1811.00982))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "This notebook is designed to be run in Amazon SageMaker. To run it (and understand what's going on), you'll need:\n",
    "\n",
    "* Basic familiarity with Python, [AWS S3](https://docs.aws.amazon.com/s3/index.html), [Amazon Sagemaker](https://aws.amazon.com/sagemaker/), and the [AWS Command Line Interface (CLI)](https://aws.amazon.com/cli/).\n",
    "* To run in **a region where Rekognition custom labelling is available** (Only N. Virginia at launch), if you plan to explore this feature.\n",
    "* To create an **S3 bucket** in the same region, and ensure the SageMaker notebook's role has access to this bucket.\n",
    "* Sufficient [SageMaker quota limits](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html#limits_sagemaker) set on your account to run GPU-accelerated training jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost and runtime\n",
    "\n",
    "Depending on your configuration, this demo may consume resources outside of the free tier but should not generally be expensive because we'll be training on a small number of images. You might wish to review the following for your region:\n",
    "\n",
    "* [Amazon SageMaker pricing](https://aws.amazon.com/sagemaker/pricing/)\n",
    "* [SageMaker Ground Truth pricing](https://aws.amazon.com/sagemaker/groundtruth/pricing/)\n",
    "* [Amazon Rekognition pricing](https://aws.amazon.com/rekognition/pricing/)\n",
    "\n",
    "The standard `ml.t2.medium` instance should be sufficient to run the notebooks.\n",
    "\n",
    "We will use GPU-accelerated instance types for training and hyperparameter optimization, and use spot instances where appropriate to optimize these costs.\n",
    "\n",
    "As noted in the step-by-step guidance, you should take particular care to delete any created SageMaker real-time prediction endpoints when finishing the demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Dependencies and configuration\n",
    "\n",
    "As usual we'll start by loading libraries, defining configuration, and connecting to the AWS SDKs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# Built-Ins:\n",
    "import csv\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "import imageio\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Local Dependencies:\n",
    "%aimport util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we configure the name and layout of your bucket, and the annotation job to set up.\n",
    "\n",
    "**If you're following this demo in a group:**, you can pool your annotations for better accuracy without spending hours annotating:\n",
    "\n",
    "* Have each group member set a different `BATCH_OFFSET` integer from 0 upwards and you'll be allocated different images to annotate\n",
    "* Later, you can *import* the other members' output manifest files to your own S3 data set.\n",
    "\n",
    "**If not:** don't worry - we already provide a 100-image set in this repository to augment your annotations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overall S3 bucket layout:\n",
    "# Note you could instead set for auto-setup: BUCKET_NAME = sagemaker.Session().default_bucket()\n",
    "BUCKET_NAME = # TODO: Put your bucket name here\n",
    "%store BUCKET_NAME\n",
    "DATA_PREFIX = \"data\" # The folder in the bucket (and locally) where we will store data\n",
    "%store DATA_PREFIX\n",
    "MODELS_PREFIX = \"models\" # The folder in the bucket where we will store models\n",
    "%store MODELS_PREFIX\n",
    "CHECKPOINTS_PREFIX = \"models/checkpoints\" # Model checkpoints can go in a subfolder of models\n",
    "%store CHECKPOINTS_PREFIX\n",
    "\n",
    "## Annotation job:\n",
    "CLASS_NAMES = [\"Boot\", \"Cat\"]\n",
    "%store CLASS_NAMES\n",
    "N_EXAMPLES_PER_CLASS = 20\n",
    "BATCH_OFFSET = 0\n",
    "BATCH_NAME = \"my-annotations\"\n",
    "\n",
    "# Note that some paths are reserved, restricting your choice of BATCH_NAME:\n",
    "data_raw_prefix = DATA_PREFIX + \"/raw\"\n",
    "data_augment_prefix = DATA_PREFIX + \"/augmentation\"\n",
    "data_batch_prefix = f\"{DATA_PREFIX}/{BATCH_NAME}\"\n",
    "test_image_folder = DATA_PREFIX + \"/test\"\n",
    "%store test_image_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just connect to the AWS SDKs we'll use, and validate the choice of S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "s3 = session.resource(\"s3\")\n",
    "bucket = s3.Bucket(BUCKET_NAME)\n",
    "smclient = session.client(\"sagemaker\")\n",
    "\n",
    "bucket_region = \\\n",
    "    session.client(\"s3\").head_bucket(Bucket=BUCKET_NAME)[\"ResponseMetadata\"][\"HTTPHeaders\"][\"x-amz-bucket-region\"]\n",
    "assert (\n",
    "    bucket_region == region\n",
    "), f\"Your S3 bucket {BUCKET_NAME} and this notebook need to be in the same region.\"\n",
    "\n",
    "if (region != \"us-east-1\"):\n",
    "    print(\"WARNING: Rekognition Custom Labels functionality is only available in us-east-1 at launch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set the goalposts with some unlabelled target data\n",
    "\n",
    "Let's start out by collecting a handful of images from around the web to illustrate what we'd like to detect.\n",
    "\n",
    "These images are not licensed and the links may break for different regions / times in future: Feel free to add your own or replace with any other images of boots and cats!\n",
    "\n",
    "Model evaluations in following notebooks will loop through each image in the `test_image_folder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p $test_image_folder\n",
    "!wget -O $test_image_folder/tabby.jpg https://images.fineartamerica.com/images-medium-large-5/1990s-ginger-and-white-tabby-cat-animal-images.jpg\n",
    "!wget -O $test_image_folder/beatbox.jpg https://midnightmusic.com.au/wp-content/uploads/2014/08/How-to-beatbox-5001.png\n",
    "!wget -O $test_image_folder/ampersand.jpg https://i.ytimg.com/vi/DsC5hNYpP9Y/maxresdefault.jpg\n",
    "!wget -O $test_image_folder/boots.jpg https://d28m5bx785ox17.cloudfront.net/v1/img/w4r1gr5IKcC9tTcJG_vsJVbyjZ_SVKuFf3YBxtrGdFs=/d/l\n",
    "!wget -O $test_image_folder/cats.jpg https://www.dw.com/image/42582511_401.jpg\n",
    "\n",
    "for test_image in os.listdir(test_image_folder):\n",
    "    display(HTML(f\"<h4>{test_image}</h4>\"))\n",
    "    util.visualize_detection(f\"{test_image_folder}/{test_image}\", [], [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Map our class names to OpenImages class IDs\n",
    "\n",
    "OpenImages defines a hierarchy of object types (e.g. \"swan\" is a subtype of \"bird\"), and references each with a class ID instead of the human-readable name.\n",
    "\n",
    "Since we want to find images containing boots and cats, our first job is to figure what OpenImages class IDs they correspond to.\n",
    "\n",
    "We start by downloading the OpenImages metadata, below.\n",
    "\n",
    "(Note we're only referencing the `test` subset of OpenImages as an easy way to keep data volumes small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download and process the Open Images annotations.\n",
    "os.makedirs(data_raw_prefix, exist_ok=True)\n",
    "!wget -O $data_raw_prefix/annotations-bbox.csv https://storage.googleapis.com/openimages/2018_04/test/test-annotations-bbox.csv \n",
    "!wget -O $data_raw_prefix/class-descriptions.csv https://storage.googleapis.com/openimages/2018_04/class-descriptions.csv \n",
    "!wget -O $data_raw_prefix/labels-hierarchy.json https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a case-insensitive lookup for our class names in the classes CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class list is really long, so let's stream it instead of loading dataframe:\n",
    "class_root_ids = { s: None for s in CLASS_NAMES }\n",
    "classes_lower_notfound = { s.lower(): s for s in CLASS_NAMES }\n",
    "with open(f\"{data_raw_prefix}/class-descriptions.csv\", \"r\") as f:\n",
    "    for row in csv.reader(f):\n",
    "        row_class_lower = row[1].lower()\n",
    "        match = classes_lower_notfound.get(row_class_lower)\n",
    "        if (match is not None):\n",
    "            class_root_ids[match] = row[0]\n",
    "            del classes_lower_notfound[row_class_lower]\n",
    "            if (len(classes_lower_notfound) == 0):\n",
    "                print(\"Class name -> root ID mapping done\")\n",
    "                break\n",
    "\n",
    "print(class_root_ids)\n",
    "if len(classes_lower_notfound):\n",
    "    raise ValueError(\n",
    "        f\"IDs not found for these class names: {[v for (k,v) in classes_lower_notfound.items()]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we recurse down the ontology from these root classes to capture any child classes.\n",
    "\n",
    "(Note that actually \"boot\" and \"cat\" are leaf nodes in OpenImages v4, but other common demos like \"bird\" are not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{data_raw_prefix}/labels-hierarchy.json\", \"r\") as f:\n",
    "    hierarchy = json.load(f)\n",
    "\n",
    "def get_all_subclasses(class_id, tree):\n",
    "    \"\"\"Get the set of `class_id` and all matching subclasses from hierarchy `tree`\"\"\"\n",
    "    def all_subtree_class_ids(subtree):\n",
    "        if (\"Subcategory\" in subtree):\n",
    "            return set([subtree[\"LabelName\"]]).union(\n",
    "                *[all_subtree_class_ids(s) for s in subtree[\"Subcategory\"]]\n",
    "            )\n",
    "        else:\n",
    "            return set([subtree[\"LabelName\"]])\n",
    "    if (tree[\"LabelName\"] == class_id):\n",
    "        return all_subtree_class_ids(tree)\n",
    "    elif \"Subcategory\" in tree:\n",
    "        return set().union(*[get_all_subclasses(class_id, s) for s in tree[\"Subcategory\"]])\n",
    "    else:\n",
    "        return set()\n",
    "\n",
    "class_id_sets = {\n",
    "    name: get_all_subclasses(class_root_ids[name], hierarchy) for name in class_root_ids\n",
    "}\n",
    "print(\"Final OpenImages class ID sets:\")\n",
    "print(class_id_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Find suitable example images\n",
    "\n",
    "Now we've looked up the full range of applicable label IDs, we can use the OpenImages annotations to extract which image IDs will be interesting for us to train on (i.e. they contain boots and/or cats).\n",
    "\n",
    "We deliberately search through the data-set in deterministic order, and only want to collect `N_EXAMPLES_PER_CLASS` images for each label but need to offset the ones we pick up by `BATCH_OFFSET` if this is non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip these images with known bad quality content:\n",
    "SKIP_IMAGES = {\"251d4c429f6f9c39\", \"065ad49f98157c8d\"}\n",
    "\n",
    "# Dict[class_name][img_id] -> [class_name, xmin, xmax, ymin, ymax]\n",
    "class_bbs = { name: defaultdict(list) for name in class_id_sets }\n",
    "\n",
    "# BATCH_OFFSET allows \n",
    "n_images_needed = N_EXAMPLES_PER_CLASS * (BATCH_OFFSET + 1)\n",
    "\n",
    "unfilled_class_names = set(CLASS_NAMES)\n",
    "with open(f\"{data_raw_prefix}/annotations-bbox.csv\", \"r\") as f:\n",
    "    for row in csv.reader(f):\n",
    "        img_id, _, cls_id, conf, xmin, xmax, ymin, ymax, *_ = row\n",
    "        if (img_id in SKIP_IMAGES):\n",
    "            continue\n",
    "        curr_unfilled_class_names = unfilled_class_names.copy()\n",
    "        for name in curr_unfilled_class_names:\n",
    "            if (cls_id in class_id_sets[name]):\n",
    "                class_bbs[name][img_id].append([name, xmin, xmax, ymin, ymax])\n",
    "                if (len(class_bbs[name]) >= n_images_needed):\n",
    "                    unfilled_class_names.remove(name)\n",
    "                    \n",
    "if (len(unfilled_class_names)):\n",
    "    print(\n",
    "        \"WARNING: Found fewer than (\"\n",
    "        + f\"{N_EXAMPLES_PER_CLASS}x{BATCH_OFFSET+1}={n_images_needed}\"\n",
    "        + \") requested images for the following classes:\\n\"\n",
    "        + \"\\n\".join([f\"{name} ({len(class_bbs[name])} images)\" for name in unfilled_class_names])\n",
    "    )\n",
    "\n",
    "bbs = defaultdict(list)\n",
    "for class_name in class_bbs:\n",
    "    # Take last N_EXAMPLES_PER_CLASS images from each class (for BATCH_OFFSET)\n",
    "    class_bbs_all_unfiltered = list(class_bbs[class_name].items())\n",
    "    class_bbs_batch = class_bbs_all_unfiltered[-N_EXAMPLES_PER_CLASS:]\n",
    "    class_bbs[class_name] = defaultdict(list, class_bbs_batch)\n",
    "    # Concatenate each class together into the overall `bbs` set\n",
    "    for (img_id, boxes) in class_bbs_batch:\n",
    "        bbs[img_id] = bbs[img_id] + boxes\n",
    "\n",
    "image_ids = bbs.keys()\n",
    "n_images = len(image_ids)\n",
    "print(f\"Selected {n_images} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Upload images and manifest file to S3\n",
    "\n",
    "We need our training image data in an accessible S3 bucket, and a [manifest](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-data-input.html) file defining for SageMaker Ground Truth (and later our model) what images are in the data set and where to find them.\n",
    "\n",
    "In the following cell, we:\n",
    "\n",
    "* Copy each identified image directly from the OpenImages repository to our bucket\n",
    "* Build up a local manifest file listing all the images\n",
    "* Upload the manifest file to the bucket\n",
    "\n",
    "This process should only take a few seconds with small data sets like we're dealing with here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{data_batch_prefix}/manifests\", exist_ok=True)\n",
    "input_manifest_loc = f\"{data_batch_prefix}/manifests/input.manifest\"\n",
    "\n",
    "with open(input_manifest_loc, \"w\") as f:\n",
    "    print(\"Copying images\", end=\"\")\n",
    "    # TODO: Delete existing folder contents?\n",
    "    for image_id in image_ids:\n",
    "        print(\".\", end=\"\")\n",
    "        dest_key = f\"{data_batch_prefix}/images/{image_id}.jpg\"\n",
    "        bucket.copy(\n",
    "            {\n",
    "                \"Bucket\": \"open-images-dataset\",\n",
    "                \"Key\": f\"test/{image_id}.jpg\"\n",
    "            },\n",
    "            dest_key\n",
    "        )\n",
    "        f.write(json.dumps({ \"source-ref\": f\"s3://{BUCKET_NAME}/{dest_key}\" }) + \"\\n\")\n",
    "    print(\"\")\n",
    "    print(f\"Images copied to s3://{BUCKET_NAME}/{data_batch_prefix}/images/\")\n",
    "\n",
    "bucket.upload_file(input_manifest_loc, input_manifest_loc)\n",
    "print(f\"Manifest uploaded to s3://{BUCKET_NAME}/{input_manifest_loc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Set up the SageMaker Ground Truth labelling job\n",
    "\n",
    "Now that our images and a manifest file listing them are ready in S3, we'll set up the Ground Truth labelling job **in the [AWS console](https://console.aws.amazon.com)**.\n",
    "\n",
    "Under *Services* go to *Amazon SageMaker*, and select *Ground Truth > Labeling Jobs* from the side-bar menu on the left.\n",
    "\n",
    "**Note:** These steps assume you've either never used SageMaker Ground Truth before, or have already set up a Private Workforce that will be suitable for this task. If you have one or more private workforces configured already, but none of them are appropriate for this task, you'll need to go to *Ground Truth > Labeling workforces* **first** to create a new one.\n",
    "\n",
    "### Job Details\n",
    "\n",
    "Click the **Create labeling job** button, and you'll be asked to specify job details as follows:\n",
    "\n",
    "* **Job name:** Choose a name to identify this labelling job, e.g. `boots-and-cats-batch-0`\n",
    "* **Label name (The override checkbox):** Consider overriding this to `labels`\n",
    "* **Input data location:** The path to the input manifest file in S3 (see output above)\n",
    "* **Output data location:** Set this just to the parent folder of the input manifests (e.g. *s3://gt-object-detect-thewsey-us-east-1/data/my-annotations*)\n",
    "* **IAM role:** If you're not sure whether your existing roles have the sufficient permissions for Ground Truth, select the options to create a new role\n",
    "* **Task type:** Image > Bounding box\n",
    "\n",
    "<img src=\"BlogImages/JobDetailsIntro.png\"/>\n",
    "\n",
    "All other settings can be left as default. Record your choices for the label name and output data location below, because we'll need these later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_groundtruth_job_name = # TODO: \"boots-and-cats-batch-0\"?\n",
    "my_groundtruth_output = f\"s3://{BUCKET_NAME}/data/my-annotations\" # TODO: **No trailing slash!**\n",
    "my_groundtruth_labels = \"labels\" # TODO: Check this matches yours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workers\n",
    "\n",
    "On the next screen, we'll configure **who** will annotate our data: Ground Truth allows you to define your own in-house *Private Workforces*; use *Vendor Managed Workforces* for specialist tasks; or use the public workforce provided by *Amazon Mechanical Turk*.\n",
    "\n",
    "Select **Private** worker type, and you'll be prompted either to select from your existing private workforces, or create a new one if none exist.\n",
    "\n",
    "To create a new private workforce if you need, simply follow the UI workflow with default settings. It doesn't matter what you call the workforce, and you can create a new Cognito User Group to define the workforce. **Add yourself** to the user pool by adding your email address: You should receive a confirmation email shortly with a temporary password and a link to access the annotation portal.\n",
    "\n",
    "Automatic data labeling is applicable only for data sets over 1000 samples, so leave this turned **off** for now.\n",
    "\n",
    "<img src=\"BlogImages/SelectPrivateWorkforce.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling Tool\n",
    "\n",
    "Since you'll be labelling the data yourself, a brief description of the task should be fine in this case. When using real workforces, it's important to be really clear in this section about the task requirements and best practices - to ensure consistency of annotations between human workers.\n",
    "\n",
    "For example: In the common case where we see a *pair* of boots from the side and one is almost entirely obscured, how should the image be annotated? Should *model* cats count, or only real ones?\n",
    "\n",
    "The most important configuration here is to set the *options* to be the same as our `CLASS_NAMES` and in the same order: **Boot, Cat**\n",
    "\n",
    "<img src=\"BlogImages/LabellingToolSetup.png\"/>\n",
    "\n",
    "Take some time to explore the other options for configuring the annotation tool; and when you're ready click \"Create\" to launch the labeling job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Label those images!\n",
    "\n",
    "Follow the link you received in your workforce invitation email to the workforce's **labelling portal**, and log in with the default password given in the email (which you'll be asked to change).\n",
    "\n",
    "If you lose the portal link, you can always retrieve it through the *Ground Truth > Labeling Workforces* menu in the SageMaker console: Near the top of the summary of private workforces.\n",
    "\n",
    "New jobs can sometimes take a minute or two to appear for workers, but you should soon see a screen like the below. Select the job and click \"Start working\" to enter the labelling tool.\n",
    "\n",
    "<img src=\"BlogImages/LabellingJobsReady.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can check on the progress of labelling jobs through the APIs as well as in the AWS console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smclient.describe_labeling_job(LabelingJobName=my_groundtruth_job_name)['LabelingJobStatus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label all the images in the tool by selecting the class and drawing boxes around the objects, and when done you will be brought back to the (now empty) jobs list screen above.\n",
    "\n",
    "It may take a few seconds after completing for the job status to update in the AWS console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: (Optional) Augment the labelled data\n",
    "\n",
    "This repository contains an example output manifest (100 images) which we can use to augment our data set and improve our model's accuracy.\n",
    "\n",
    "The script below **imports** the data to our bucket, since we don't have access to the S3 bucket where the annotation job was originally performed. We extract the OpenImages image ID/filename from each entry in the manifest; copy this image into our bucket; and create an updated output manifest with the new `source-ref`s.\n",
    "\n",
    "The code includes some tips in case you want to re-purpose it for other data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function definition:\n",
    "def import_manifest_file(manifest_file_path, updated_manifest_file_path, target_prefix):\n",
    "    \"\"\"Import a Ground Truth output.manifest file to the current S3 bucket\n",
    "    Parameters:\n",
    "    ----------\n",
    "    manifest_file_path: str\n",
    "        Local relative path to an output manifest which was created against a different S3 bucket\n",
    "    updated_manifest_file_path: str\n",
    "        Local relative path (and S3 key) to store the new manifest with updated S3 keys\n",
    "    target_prefix: str\n",
    "        Place in current S3 bucket to store the images and updated manifest\n",
    "    \"\"\"\n",
    "    with open(manifest_file_path, \"r\") as f_source:\n",
    "        with open(updated_manifest_file_path, \"w\") as f_target:\n",
    "            print(\"Copying images\", end=\"\")\n",
    "            for line in f_source:\n",
    "                print(\".\", end=\"\")\n",
    "                datum = json.loads(line)\n",
    "                image_file = datum[\"source-ref\"].rpartition(\"/\")[2]\n",
    "                dest_key = f\"{target_prefix}/images/{image_file}\"\n",
    "                bucket.copy(\n",
    "                    {\n",
    "                        \"Bucket\": \"open-images-dataset\",\n",
    "                        \"Key\": f\"test/{image_file}\"\n",
    "                    },\n",
    "                    dest_key\n",
    "                )\n",
    "                datum[\"source-ref\"] = f\"s3://{BUCKET_NAME}/{dest_key}\"\n",
    "                f_target.write(json.dumps(datum) + \"\\n\")\n",
    "    print(\"\")\n",
    "    # Upload the updated manifest to S3 for our use in model fitting:\n",
    "    bucket.upload_file(updated_manifest_file_path, updated_manifest_file_path)\n",
    "    print(f\"Augmentation manifest uploaded to:\\ns3://{BUCKET_NAME}/{updated_manifest_file_path}\")\n",
    "\n",
    "\n",
    "# Import the augmentation data included in the repository:\n",
    "augment_manifest_loc = f\"{data_augment_prefix}/manifests/output/output.updated.manifest\"\n",
    "import_manifest_file(\n",
    "    f\"{data_augment_prefix}/manifests/output/output.manifest\",\n",
    "    augment_manifest_loc,\n",
    "    data_augment_prefix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If wanting to import another manifest file:\n",
    "\n",
    "# Ensure the folder creation, if you need to:\n",
    "#os.makedirs(\"data/[SUBFOLDERNAME]/manifests/output\", exist_ok=True)\n",
    "\n",
    "# Download the output.manifest to our standard location:\n",
    "#!aws s3 cp s3://WHEREVER-THE-OUTPUT.MANIFEST-LIVES ./data/[SUBFOLDERNAME]/manifests/output/output.manfiest\n",
    "# (Or copy it some other way if you don't have access via S3)\n",
    "\n",
    "# Import the file:\n",
    "# import_manifest_file(\n",
    "#     \"data/[SUBFOLDERNAME]/manifests/output/output.manifest\",\n",
    "#     \"data/[SUBFOLDERNAME]/manifests/output/output.updated.manifest\",\n",
    "#     \"data/[SUBFOLDERNAME]\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Consolidate the labeling results\n",
    "Now that our own labelling job is complete and we've imported other data to augment our data-set, we'll consolidate the batches together into a single combined manifest file.\n",
    "\n",
    "First, validate and adjust the below configuration for your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_jobs = {\n",
    "    # The batch you just annotated:\n",
    "    # (Comment out this entry if you skipped creating & completing the Ground Truth job)\n",
    "    BATCH_NAME: {\n",
    "        # The label name you selected in the console:\n",
    "        # (GroundTruth will set this equal to the job name by default)\n",
    "        \"label_name\": my_groundtruth_labels, # TODO\n",
    "        # This may be different if you selected something else for the output path other than:\n",
    "        # s3://{BUCKET}/{batch_data_prefix}\n",
    "        \"manifest\": f\"{my_groundtruth_output}/{my_groundtruth_job_name}/manifests/output/output.manifest\"\n",
    "    },\n",
    "    \n",
    "    # The augmentation batch provided in the repo:\n",
    "    # (Comment out this entry if you skipped importing this)\n",
    "    \"augmentation\": {\n",
    "        \"label_name\": \"labels\",\n",
    "        \"manifest\": f\"s3://{BUCKET_NAME}/data/augmentation/manifests/output/output.updated.manifest\"\n",
    "    }\n",
    "    \n",
    "    # Did you import any other batches?\n",
    "}\n",
    "\n",
    "print(json.dumps(annotation_jobs, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the contents from each manifest and consolidate in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_job_ids = list(annotation_jobs.keys())\n",
    "\n",
    "for job_id in annotation_jobs:\n",
    "    manifest_bucket, _, manifest_path = annotation_jobs[job_id][\"manifest\"][len(\"s3://\"):].partition(\"/\")\n",
    "    manifest_folder, _, manifest_file = manifest_path.rpartition(\"/\")\n",
    "    os.makedirs(manifest_folder, exist_ok=True)\n",
    "    s3.Bucket(manifest_bucket).download_file(manifest_path, manifest_path)\n",
    "    with open(manifest_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f.readlines()]\n",
    "        # Standardize label names:\n",
    "        for datum in data:\n",
    "            datum[\"labels\"] = datum.pop(annotation_jobs[job_id][\"label_name\"])\n",
    "        annotation_jobs[job_id][\"data\"] = data\n",
    "        print(f\"{len(data)} samples from annotation set {job_id}\")\n",
    "\n",
    "combined_manifest_data = [datum for job_id in annotation_jobs for datum in annotation_jobs[job_id][\"data\"]]\n",
    "print(f\"Got {len(combined_manifest_data)} total samples from {len(annotation_job_ids)} sets\")\n",
    "\n",
    "# The standardization above means these are always the attributes training will care about:\n",
    "attribute_names = [\"source-ref\", \"labels\"]\n",
    "%store attribute_names\n",
    "\n",
    "# For illustration, this is what an entry in our combined manifest looks like:\n",
    "combined_manifest_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Split training vs validation and upload final manifests\n",
    "\n",
    "Now we have all our consolidated label sets (and all the referenced images uploaded in our S3 bucket), the final step is to split training vs validation data and upload a manifest for each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle output in place.\n",
    "np.random.shuffle(combined_manifest_data)\n",
    "    \n",
    "n_samples_total = len(combined_manifest_data)\n",
    "train_test_split_index = round(n_samples_total*0.8)\n",
    "\n",
    "train_data = combined_manifest_data[:train_test_split_index]\n",
    "validation_data = combined_manifest_data[train_test_split_index:]\n",
    "\n",
    "n_samples_training = len(train_data)\n",
    "%store n_samples_training\n",
    "n_samples_validation = len(validation_data)\n",
    "%store n_samples_validation\n",
    "\n",
    "with open(f\"{DATA_PREFIX}/train.manifest\", \"w\") as f:\n",
    "    for line in train_data:\n",
    "        f.write(json.dumps(line))\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "with open(f\"{DATA_PREFIX}/validation.manifest\", \"w\") as f:\n",
    "    for line in validation_data:\n",
    "        f.write(json.dumps(line))\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "bucket.upload_file(f\"{DATA_PREFIX}/train.manifest\", f\"{DATA_PREFIX}/train.manifest\")\n",
    "print(\"Training manifest uploaded to:\\n\" + f\"s3://{BUCKET_NAME}/{DATA_PREFIX}/train.manifest\")\n",
    "bucket.upload_file(f\"{DATA_PREFIX}/validation.manifest\",f\"{DATA_PREFIX}/validation.manifest\")\n",
    "print(\"Validation manifest uploaded to:\\n\" + f\"s3://{BUCKET_NAME}/{DATA_PREFIX}/validation.manifest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "Phew! That felt like a lot of work, but a lot of the steps were hacks for our example:\n",
    "\n",
    "* To find raw image data for our targets (boots and cats), we mapped our class names to the public OpenImages dataset and used their existing annotations to find relevant images.\n",
    "* To get a decent data volume without spending forever annotating in the workshop, we merged our Ground Truth annotation results with other augmentation sets.\n",
    "\n",
    "The useful points to remember are:\n",
    "\n",
    "* SageMaker Ground Truth (and as we'll see later, many of the built-in algorithms as well) uses **augmented manifests** to define annotated image datasets.\n",
    "* These manifests are just plain text [JSON Lines](http://jsonlines.org/) files that we can also edit in our own code to do whatever we like from importing/exporting annotations, to stitching together datasets as we did here.\n",
    "* Once the input manifest is prepared, it only takes a few clicks to define workforce teams and annotation jobs in SageMaker Ground Truth: Which supports other built-in and even custom annotation workflows for a variety of data types and tasks.\n",
    "\n",
    "Although we didn't use it here due to the dataset size, the [automated labelling](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-automated-labeling.html) feature can drastically cut annotation costs and time on bigger data-sets for the tasks where it's supported (including object detection).\n",
    "\n",
    "Ground Truth supports validation workflows (typically much faster for humans) as well as labelling; which can be combined with automated labelling in light of the importance of good quality ground truth input to effective machine learning.\n",
    "\n",
    "In the follow-on notebooks, we'll use the composite training and validation datasets we created here to fit a variety of models and compare their performance. Let's move on to notebook 2(a)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
